---
title: "Loss estimation"
date: "`r Sys.Date()`"
output: pdf_document
---
## Introduction
The goal is to perform loss estimation using two methods: 
hold-out estimation and cross-validation. We want to analyze how these
estimators of the model's risk deviate from the actual risk by comparing the 
estimated to the true risk. 


### Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(echo = TRUE, results = "hold")
library(aod)
library(ggplot2)
library(grid)
library(gridExtra)
library(caTools)
library(party)
library(dplyr)
library(magrittr)
library(tidyr)
library(rpart)
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
```

In this section, we prepare utils functions for calculating standard error and
confidence intervals. 

```{r utils}
std_err <- function(x){
    mi <- mean(x)
    m <- length(x)
    var_l <- 1/(m-1)*sum((x-mi)^2)
    sqrt(var_l/m)    
}

conf_interval <- function(x, alpha){
    n <- length(x)
    mean_x <- mean(x)
    std_x <- std_err(x)
    z <- qnorm(1-alpha/2)
    c(mean_x - z*std_x, mean_x + z*std_x)
}
```

## Methodology
For this task, we utilize datasets generated by the function provided below, 
with average of the negative log loss serving as our measure of risk. We employ 
logistic regression model for our experiments.
```{r methodology}
toy_data <- function(n, seed = NULL) {
    set.seed(seed)
    x <- matrix(rnorm(8 * n), ncol = 8)
    z <- 0.4 * x[,1] - 0.5 * x[,2] + 1.75 * x[,3] - 0.2 * x[,4] + x[,5]
    y <- runif(n) > 1 / (1 + exp(-z))
    return (data.frame(x = x, y = y))
}

log_loss <- function(y, p) {
    -(y * log(p) + (1 - y) * log(1 - p))
}
```

### True risk proxy
To compute the true risk proxy, we generate a very large set that allows an 
accurate representation of the DGP. To show that 100000 samples is enough for 
computing true risk proxy, we look at variance and confidence intervals of risks
that different datasets of this size produce. Note that we train the model with 
a dataset of size 50, because this is the size of the smallest dataset used in 
our experiments.

```{r error_calc}
df_50 <- toy_data(50, 0)
h <- glm(y ~ ., data = df_50, family = binomial(link = "logit"))
risks <- c()
for (i in 1:500){
  df_dgp <- toy_data(100000, i**2)
  pred_dgp <-  predict(h, newdata = df_dgp, type = "response")
  losses <- log_loss(df_dgp$y, pred_dgp)
  risks <- c(risks, mean(losses))
}
```

```{r}
conf_interval(risks, alpha=0.01)
```

The bounds of the constructed 99% confidence interval differ at the fourth 
decimal. Therefore, we can reliably assume that any estimate made with this 
dataset would be accurate up to third decimal.


```{r dgp dataset}
df_dgp <- toy_data(100000, 0)
```

### Holdout estimation

```{r holdout_est}
# generate a toy dataset with 50 observations
df_train <- toy_data(50, 0)

# train a logistic regression model
h <- glm(y ~ ., data = df_train, family = binomial(link = "logit"))

# compute the true risk proxy for h using the df_dgp
pred_dgp <- predict(h, newdata = df_dgp, type = "response")
true_risk <- mean(log_loss(df_dgp$y, pred_dgp))

diff_risk <- c()
contains_tr <- c()
std_errs <- c()
for (i in 1:1000) {
    df_td_test <- toy_data(50, i**2)
    pred_td_test <- predict(h, newdata = df_td_test, type = "response")
    risk_estimate <- mean(log_loss(df_td_test$y, pred_td_test))
    # compute difference between risk estimate and true risk
    diff_risk <- c(diff_risk, risk_estimate - true_risk)
    
    # compute standard error
    se_re <- std_err(log_loss(df_td_test$y, pred_td_test))
    std_errs <- c(std_errs, se_re)
    
    # record if the 95% CI contains the true risk
    ci_re <- conf_interval(log_loss(df_td_test$y, pred_td_test), 0.05)
    ci_contains_tr <- ci_re[1] <= true_risk & true_risk <= ci_re[2]
    contains_tr <- c(contains_tr, ci_contains_tr)
}

# compute the true risk of always making 0.5 - 0.5 predictions
pred_baseline <- rep(0.5, length(df_dgp$y))
baseline_risk <- mean(log_loss(df_dgp$y, pred_baseline))
```

```{r, echo=FALSE, out.width="50%"}
ggplot(data = data.frame(diff_risk = diff_risk), aes(x = diff_risk)) +
    geom_density() +
    xlab("Risk estimate - True risk proxy") +
    ylab("Density")
```

```{r, echo=FALSE}
sprintf("True risk proxy: %.4f", true_risk)
sprintf("Mean difference: %.4f", mean(diff_risk))
sprintf("0.5-0.5 baseline true risk: %.4f", mean(baseline_risk))
sprintf("Median standard error: %.4f", median(std_errs))
sprintf("Percentage of 95CI that contain the true risk proxy: %.1f", 
        mean(contains_tr) * 100)
```
In the initial experimental setup, we implement an *idealized* form of holdout 
estimation. We allocate a dedicated set for training the model and an additional
set comprising IID samples from the DGP for test set and estimating risk.

From the plotted data, we observe that the distribution of differences between 
the risk estimate and the true risk proxy is right-skewed. This skewness 
suggests a slight tendency towards overestimating the risk, consequently 
underestimating the model's performance. However, the mean of these differences
is close to zero, indicating an unbiased estimator. This is because, we evaluate 
the actual model of interest (the model we would deploy) on a completely 
independent set of IID observations from our DGP.

If the training set was larger, we would get a smaller risk. We expect that the 
performance of the model does not decrease with more training data.

Expanding the size of the testing set would yield a smaller standard error in 
the risk estimator, thereby providing a more reliable estimate. The evaluation 
would be less influenced by the outliers and single data instances. Also, 
evaluation with larger test set better represents the model's ability to 
generalize.

We can employ this *idealized* case of holdout estimation if we can allocate a 
completely separate test set and if our training set is sufficiently large to 
ensure that we do not need the model to be trained on all available data. In 
practice, this is rarely the case, and we want our models to be trained on the 
whole dataset.

### Overestimation of the deployed model's risk

```{r overestimation}
tr_diffs <- c()
for (i in 1:50) {
    # generate two toy datasets
    df_td1 <- toy_data(50, 2*i)
    df_td2 <- toy_data(50, 2*i + 1)
    df_td <- rbind(df_td1, df_td2)

    # train model h1 using a learner and the first dataset only
    h1 <- glm(y ~ ., data = df_td1, family = binomial(link = "logit"))

    # train model h2 using the same learner and both datasets
    h2 <- glm(y ~ ., data = df_td, family = binomial(link = "logit"))

    # compute the true risk proxies for h1 and h2 using the df_dgp
    pred_h1 <- predict(h1, newdata = df_dgp, type = "response")
    true_risk_h1 <- mean(log_loss(df_dgp$y, pred_h1))
    pred_h2 <- predict(h2, newdata = df_dgp, type = "response")
    true_risk_h2 <- mean(log_loss(df_dgp$y, pred_h2))
    
    tr_diff <- true_risk_h1 - true_risk_h2
    tr_diffs <- c(tr_diffs, tr_diff)
}
summary(tr_diffs)
```

In this section, we aim to validate the claim that more data typically means
that the learner will produce a model with lower true risk. To do so, we compare
the true risk proxies of a model trained on 50 samples from our DGP with those
of a model trained on 100 samples from our DGP (additional 50 samples).
The summary of these differences is provided above, where we can notice that the
mean exceeds the median, suggesting a right-skewed distribution.

In practice, we would encounter the following example: we split our dataset
into two sets, each containing 50 samples for training and testing. After
building a model using the training set, we evaluate its performance using the
test set. Referring to the experiment above, we can conclude that retraining the
model on the entire dataset would likely yield a smaller true risk. It's worth
noting that conducting the evaluation on the test set, and later training the
model on the whole dataset, would likely overestimate the risk of the deployed
model.

If the datasets were larger, the difference would be less observed, as the
training dataset could be large enough to train the model sufficiently -
resulting in improved performance for both models. The true risk and the
estimated risk would be smaller, and bias and variance of the estimator would
also decrease.

With smaller datasets, estimating the model's performance becomes less reliable,
leading to increased bias, as the performance and evaluation heavily rely on the
individual samples within the separate sets. In such cases, both the true risk
and the risk estimate would likely increase.


### Loss estimator variability due to split variability

```{r split_var}
# generate a toy dataset with 100 observations
df_td <- toy_data(100, 0)
# train logistic regression model
h0 <- glm(y ~ ., data = df_td, family = binomial(link = "logit"))
pred_h0 <- predict(h0, newdata = df_dgp, type = "response")
tr_h0 <- mean(log_loss(df_dgp$y, pred_h0))

risk_diffs <- c()
ci_contains_tr_list <- c()
std_errs <- c()
for (i in 1:1000){
    # split the dataset into 50 train and 50 test observations
    train_idx <- sample(1:100, 50)
    df_td_train <- df_td[train_idx,]
    df_td_test <- df_td[-train_idx,]
  
    # train model h using the train set
    h <- glm(y ~ ., data = df_td_train, family = binomial(link = "logit"))
    
    # estimate risk using test set
    pred_test <- predict(h, newdata = df_td_test, type = "response")
    loss_test <- log_loss(df_td_test$y, pred_test)
    risk_test <- mean(loss_test)

    risk_diffs <- c(risk_diffs, risk_test - tr_h0)
    
    # compute standard error
    se <- std_err(loss_test)
    std_errs <- c(std_errs, se)
    
    # record if the 95% CI of the estimate contains the true risk of h0
    ci <- conf_interval(loss_test, 0.05)
    ci_contains_tr <- ci[1] <= tr_h0 & tr_h0 <= ci[2]
    ci_contains_tr_list <- c(ci_contains_tr_list, ci_contains_tr)
}
```

```{r, echo=FALSE, out.width="50%"}
ggplot(data = data.frame(risk_diffs = risk_diffs), aes(x = risk_diffs)) +
    geom_density() +
    xlab("Risk estimate - True risk proxy") +
    ylab("Density")
```

```{r, echo=FALSE}
sprintf("True risk proxy: %.4f", tr_h0)
sprintf("Mean difference: %.4f", mean(risk_diffs))
sprintf("Median standard error: %.4f", median(std_errs))
sprintf("Percentage of 95CI that contain the true risk proxy: %.1f",
        mean(ci_contains_tr_list) * 100)
```
From the computed statistics, we observe that the mean difference between the
estimated and true risk is approximately 0.2428, suggesting that this holdout
estimation tends to overestimate the true risk. In the plotted data, there's a
noticeable long tail, indicating significant overestimates of risk in some
dataset splits. To mitigate the impact of such cases and obtain a more robust
estimator, we repeat the modeling process multiple times, each time using a
different training-test split.

With a larger dataset, the splitting process would become more reliable, and
individual instances would have less influence on both training and evaluation.
Consequently, this would result in a shorter tail in the plot above.
Additionally, as mentioned earlier, a larger dataset effectively provides a
larger training set, which would decrease the risk.

Decreasing the proportion of the training set would introduce bias in the
estimates because the training set would be relatively small compared to the
combined set used to train the deployed model (training + test).
Conversely, increasing the proportion of the training set would reduce bias,
but it would also increase the variance of the estimator. This is because the
risk estimator would become more sensitive to individual test instances, making
it more sensitive to noise.

### Cross validation

```{r cross_val}
cross_validation <- function(k, data, iter){
    # function for performing cross validation using logistic regression
    data$ixs <- 1:nrow(data)
    # shuffle the dataset
    shuffled_indices <- sample(1:nrow(data))
    shuffled_data <- data[shuffled_indices, ]
    # divide the data into k folds
    folds = split(shuffled_data, cut(1:nrow(shuffled_data), breaks = k,
                                     labels = FALSE))
    risks <- c()
    losses <- data.frame()
    for (j in 1:k){
        test_set <- folds[[j]]
        train_folds <- folds[-j]
        train_set <- do.call(rbind, train_folds)
        # train the model on the train folds
        h <- glm(y ~ ., data = train_set[, -ncol(train_set)],
                 family = binomial(link = "logit"))
        # test the model on the test fold
        pred_test <- predict(h, newdata = test_set[, -ncol(train_set)],
                             type = "response")
        # compute loss for the test set
        loss_test <- log_loss(test_set$y, pred_test)
        losses <- rbind(losses, data.frame(ixs = test_set$ixs,
                                           loss = loss_test))
    }
    losses <- losses[order(losses$ixs),]
    list(losses = losses)
}
```

```{r cv_try_out}
res_df_2 <- data.frame()
res_df_4 <- data.frame()
res_df_10 <- data.frame()
res_df_100 <- data.frame()
res_df_20x10 <- data.frame()

for (i in 1:500){
    data <- toy_data(100, i**2)
    # train the model on 100 observation and compute true risk proxy
    h0 <- glm(y ~ ., data = data, family = binomial(link = "logit"))
    pred_h0 <- predict(h0, newdata = df_dgp, type = "response")
    tr_h0 <- mean(log_loss(df_dgp$y, pred_h0))
    
    # estimate h0's risk with 2-fold, 4-fold, 10-fold, LOOCV and 20-rep-10-fold
    # cross validation
    for (k in c(2, 4, 10, 100)){
        res <- cross_validation(k, data, i)
        losses <- res$losses$loss
        risk_estimate <- mean(losses)
        # compute the difference between the estimate risk and true risk proxy
        risk_diff <- risk_estimate - tr_h0
        # compute standard error
        se <- std_err(losses)
        ci <- conf_interval(losses, 0.05)
        # record if the 95% CI of the estimate contains the true risk of h 0
        ci_contains_tr <- ci[1] <= tr_h0 & tr_h0 <= ci[2]
        
        # save results to the corresponding data frame
        if (k == 2){
            res_df_2 <- rbind(res_df_2, data.frame(risk_diff = risk_diff,
                                     se = se, ci_contains_tr = ci_contains_tr))
        } else if (k == 4){
            res_df_4 <- rbind(res_df_4, data.frame(risk_diff = risk_diff,
                                     se = se, ci_contains_tr = ci_contains_tr))
        } else if (k == 10){
            res_df_10 <- rbind(res_df_10, data.frame(risk_diff = risk_diff,
                                     se = se, ci_contains_tr = ci_contains_tr))
        } else if (k == 100){
            res_df_100 <- rbind(res_df_100, data.frame(risk_diff = risk_diff,
                                     se = se, ci_contains_tr = ci_contains_tr))
        }
    }
    
    # perform estimation with 20-rep-10-fold cross validation
    losses_with_rep <- rep(0, 100)
    for (rep in 1:20){
        res <- cross_validation(10, data, i)
        losses_with_rep <- losses_with_rep + res$losses$loss
    }
    # average losses for 20 repetitions
    losses_with_rep <- losses_with_rep / 20
    risk_estimate <- mean(losses_with_rep)
    # compute the difference between the estimate risk and true risk proxy
    risk_diff <- risk_estimate - tr_h0
    # compute standard error
    se <- std_err(losses_with_rep)
    # record if the 95% CI of the estimate contains the true risk of h 0
    ci <- conf_interval(losses_with_rep, 0.05)
    ci_contains_tr <- ci[1] <= tr_h0 & tr_h0 <= ci[2]
    # save results to the corresponding data frame
    res_df_20x10 <- rbind(res_df_20x10, data.frame(risk_diff = risk_diff,
                                     se = se, ci_contains_tr = ci_contains_tr))
}
```

```{r, echo=FALSE}
res_df <- data.frame("2-fold"=res_df_2$risk_diff,
                 "4-fold"=res_df_4$risk_diff,
                 "10-fold"=res_df_10$risk_diff,
                 "loocv"=res_df_100$risk_diff,
                 "10-fold-20-rep"=res_df_20x10$risk_diff, check.names=FALSE)
```

```{r, echo=FALSE}
res_df %>%
  gather(key = "fold", value = "risk_diff") %>%
  ggplot(aes(x = risk_diff)) +
  geom_density() +
  facet_wrap(~factor(fold, c("2-fold", "4-fold", "10-fold",
                             "10-fold-20-rep", "loocv"))) +
  xlim(-0.5, 7) +
  ylim(0, 6) +
  labs(x="Risk estimate - True risk proxy")
```

```{r, echo=FALSE}
fold_names <- c("2-fold", "4-fold", "10-fold", "20 repetitions of 10-fold", "loocv")
fn_idx <- 1
for (df in list(res_df_2, res_df_4, res_df_10, res_df_20x10, res_df_100)){
    print(sprintf(fold_names[fn_idx]))
    fn_idx  <- fn_idx + 1
    print(sprintf("Mean difference: %.4f", mean(df$risk_diff)))
    print(sprintf("Median standard error: %.4f", median(df$se)))
    print(sprintf("Percentage of 95CI that contain the true risk proxy: %.1f",
        mean(df$ci_contains_tr) * 100))
}
```
The results indicate that employing cross-validation with a greater number of
folds gives more accurate risk estimates with reduced standard errors. When
using a smaller number of folds (thus resulting in a smaller training set),
certain splits exhibit a significant difference between the estimated risk and
the true risk proxy. This can be observed in the long right tails in the plots
for 2-fold and 4-fold cross-validation.

In the case of 20 repetitions of 10-fold cross-validation, we mitigate the
increase in variance due to choice of partitions, by repeating the process for
different splits and averaging computed losses.

It has been shown that for stable models, leave-one-out cross-validation (LOOCV)
serves as the best risk estimator, a finding that holds also in this scenario.
However in practice, unless the dataset is relatively small, LOOCV is not used
due to its computational demands. Therefore, considering the advantages of
repeated cross-validation, it may be a better choice in practice.